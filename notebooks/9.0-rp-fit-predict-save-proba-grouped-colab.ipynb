{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1612482607466,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "I2sl9W3Au1hZ"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1612482608263,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "QyMvK7e8u1hg",
    "outputId": "0b2f2ca7-069f-4a6b-b4b8-d88265cffd5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version is: 1.1.5\n",
      "numpy version is: 1.19.5\n",
      "matplotlib version is: 3.2.2\n",
      "sklearn version is: 0.22.2.post1\n",
      "xgboost version is: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Print out packages versions\n",
    "print(f'pandas version is: {pd.__version__}')\n",
    "print(f'numpy version is: {np.__version__}')\n",
    "print(f'matplotlib version is: {matplotlib.__version__}')\n",
    "print(f'sklearn version is: {sklearn.__version__}')\n",
    "print(f'xgboost version is: {xgb.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE9_Q2bsu1hi"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1612482609544,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "m1x1Oo6ju1hi"
   },
   "outputs": [],
   "source": [
    "def replace_nan_inf(df, value=None):\n",
    "    \"\"\"\n",
    "    Replace missing and infinity values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with values to be replaced.\n",
    "\n",
    "    value : int, float\n",
    "        Value to replace any missing or numpy.inf values. Defaults to numpy.nan\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with missing and infinity values replaced with -999.\n",
    "\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        value = np.nan\n",
    "    return df.replace(to_replace=[np.nan, np.inf, -np.inf],\n",
    "                      value=value)\n",
    "\n",
    "\n",
    "def shift_concat(df, periods=1, fill_value=None):\n",
    "    \"\"\"\n",
    "    Build dataframe of shifted index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with columns to be shifted.\n",
    "    periods : int\n",
    "        Number of periods to shift. Should be positive.\n",
    "    fill_value : object, optional\n",
    "        The scalar value to use for newly introduced missing values. Defaults\n",
    "        to numpy.nan.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Shifted dataframes concatenated along columns axis.\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    Based on Paulo Bestagini's augment_features_window from SEG 2016 ML\n",
    "    competition.\n",
    "    https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try01.ipynb\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    Shift df by one period and concatenate.\n",
    "\n",
    "    >>> df = pd.DataFrame({'gr': [1.1, 2.1], 'den': [2.1, 2.2]})\n",
    "    >>> shift_concat(df)\n",
    "        gr_shifted_1  den_shifted_1  gr   den  gr_shifted_-1   den_shifted_-1\n",
    "    0      NaN            NaN        1.1  2.1      2.1             2.2\n",
    "    1      1.1            2.1        2.1  2.2      NaN             NaN\n",
    "\n",
    "    \"\"\"\n",
    "    if fill_value is None:\n",
    "        fill_value = np.nan\n",
    "\n",
    "    dfs = []\n",
    "    for period in range(periods, -1*periods - 1, -1):\n",
    "\n",
    "        if period == 0:\n",
    "            dfs.append(df)\n",
    "            continue\n",
    "\n",
    "        df_shifted = df.shift(period, fill_value=fill_value)\n",
    "\n",
    "        df_shifted.columns = [f'{col}_shifted_{str(period)}'\n",
    "                              for col in df_shifted.columns]\n",
    "\n",
    "        dfs.append(df_shifted)\n",
    "\n",
    "    return pd.concat(dfs, axis=1)\n",
    "\n",
    "\n",
    "def gradient(df, depth_col):\n",
    "    \"\"\"\n",
    "    Calculate the gradient for all features along the provided `depth_col`\n",
    "    column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with columns to be used in the gradient calculation.\n",
    "    depth_col : str\n",
    "        Dataframe column name to be used as depth reference.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Gradient of `df` along `depth_col` column. The depth column is not in\n",
    "        the output dataframe.\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    Based on Paulo Bestagini's augment_features_window from SEG 2016 ML\n",
    "    competition.\n",
    "    https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try01.ipynb\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    Calculate gradient of columns along `md`.\n",
    "\n",
    "    >>> df = pd.DataFrame({'gr': [100.1, 100.2, 100.3],\n",
    "                          'den': [2.1, 2.2, 2.3],\n",
    "                          'md': [500, 500.5, 501]})\n",
    "    >>> gradient(df, 'md')\n",
    "        gr  den\n",
    "    0  NaN  NaN\n",
    "    1  0.2  0.2\n",
    "    2  0.2  0.2\n",
    "\n",
    "    \"\"\"\n",
    "    depth_diff = df[depth_col].diff()\n",
    "\n",
    "    denom_zeros = np.isclose(depth_diff, 0)\n",
    "    depth_diff[denom_zeros] = 0.001\n",
    "\n",
    "    df_diff = df.drop(depth_col, axis=1)\n",
    "    df_diff = df_diff.diff()\n",
    "\n",
    "    # Add suffix to column names\n",
    "    df_diff.columns = [f'{col}_gradient' for col in df_diff.columns]\n",
    "\n",
    "    return df_diff.divide(depth_diff, axis=0)\n",
    "\n",
    "\n",
    "def shift_concat_gradient(df, depth_col, well_col, cat_cols, periods=1, fill_value=None):\n",
    "    \"\"\"\n",
    "    Augment features using `shif_concat` and `gradient`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with columns to be augmented.\n",
    "    depth_col : str\n",
    "        Dataframe column name to be used as depth reference.\n",
    "    well_col : str\n",
    "        Dataframe column name to be used as well reference.\n",
    "    cat_cols: list of str\n",
    "        Encoded column names. The gradient calculation is not applied to these\n",
    "        columns.\n",
    "    periods : int\n",
    "        Number of periods to shift. Should be positive.\n",
    "    fill_value : object, optional\n",
    "        The scalar value to use for newly introduced missing values. Defaults\n",
    "        to numpy.nan.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Augmented dataframe.\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    Based on Paulo Bestagini's augment_features_window from SEG 2016 ML\n",
    "    competition.\n",
    "    https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try01.ipynb\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    Augment features of `df` by shifting and taking the gradient.\n",
    "\n",
    "    >>> df = pd.DataFrame({'gr': [100.1, 100.2, 100.3, 20.1, 20.2, 20.3],\n",
    "                          'den': [2.1, 2.2, 2.3, 1.7, 1.8, 1.9],\n",
    "                           'md': [500, 500.5, 501, 1000, 1000.05, 1001],\n",
    "                         'well': [1, 1, 1, 2, 2, 2]})\n",
    "    >>> shift_concat_gradient(df, 'md', 'well', periods=1, fill_value=None)\n",
    "        gr_shifted_1  den_shifted_1     gr    den  ...  well   md    gr_gradient  den_gradient\n",
    "    0         NaN          NaN         100.1  2.1  ...   1   500.00        NaN           NaN\n",
    "    1       100.1          2.1         100.2  2.2  ...   1   500.50   0.200000      0.200000\n",
    "    2       100.2          2.2         100.3  2.3  ...   1   501.00   0.200000      0.200000\n",
    "    3         NaN          NaN          20.1  1.7  ...   2  1000.00        NaN           NaN\n",
    "    4        20.1          1.7          20.2  1.8  ...   2  1000.05   2.000000      2.000000\n",
    "    5        20.2          1.8          20.3  1.9  ...   2  1001.00   0.105263      0.105263\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO 'Consider filling missing values created here with DataFrame.fillna'\n",
    "\n",
    "    # Columns to apply gradient operation\n",
    "    cat_cols.append(well_col)\n",
    "    gradient_cols = [col for col in df.columns if col not in cat_cols]\n",
    "\n",
    "    # Don't shift depth\n",
    "    depth = df.loc[:, depth_col]\n",
    "\n",
    "    grouped = df.groupby(well_col, sort=False)\n",
    "\n",
    "    df_aug_groups = []\n",
    "    for name, group in grouped:\n",
    "        shift_cols_df = group.drop([well_col, depth_col], axis=1)\n",
    "\n",
    "        group_shift = shift_concat(shift_cols_df,\n",
    "                                   periods=periods,\n",
    "                                   fill_value=fill_value)\n",
    "\n",
    "        # Add back the well name and depth\n",
    "        group_shift[well_col] = name\n",
    "        group_shift[depth_col] = depth\n",
    "\n",
    "        group_gradient = group.loc[:, gradient_cols]\n",
    "\n",
    "        group_gradient = gradient(group_gradient, depth_col)\n",
    "\n",
    "        group_aug = pd.concat([group_shift, group_gradient], axis=1)\n",
    "\n",
    "        df_aug_groups.append(group_aug)\n",
    "\n",
    "    return pd.concat(df_aug_groups)\n",
    "\n",
    "\n",
    "def score(y_true, y_pred, scoring_matrix):\n",
    "    \"\"\"\n",
    "    Competition scoring function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : pandas.Series\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : pandas.Series\n",
    "        Estimated targets as returned by a classifier.\n",
    "    scoring_matrix : numpy.array\n",
    "        Competition scoring matrix.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    float\n",
    "        2020 FORCE ML lithology competition custome score.\n",
    "\n",
    "    \"\"\"\n",
    "    S = 0.0\n",
    "\n",
    "    for true_val, pred_val in zip(y_true, y_pred):\n",
    "        S -= scoring_matrix[true_val, pred_val]\n",
    "\n",
    "    return S/y_true.shape[0]\n",
    "\n",
    "\n",
    "def show_evaluation(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Print model performance and evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : pandas.Series\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred: pandas.Series\n",
    "        Estimated targets as returned by a classifier.\n",
    "\n",
    "    \"\"\"\n",
    "    print(f'Competition score: {score(y_true, y_pred)}')\n",
    "    print(f'Accuracy: {accuracy_score(y_true, y_pred)}')\n",
    "    print(f'F1: {f1_score(y_true, y_pred, average=\"weighted\")}')\n",
    "\n",
    "\n",
    "def build_encoding_map(series):\n",
    "    \"\"\"\n",
    "    Build dictionary with the mapping of series unique values to encoded\n",
    "    values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : pandas.Series\n",
    "        Series with categories to be encoded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mapping : dict\n",
    "        Dictionary mapping unique categories in series to encoded values.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    label_encode_columns : Label encode a dataframe categorical columns.\n",
    "\n",
    "    \"\"\"\n",
    "    unique_values = series.unique()\n",
    "\n",
    "    mapping = {original: encoded\n",
    "               for encoded, original in enumerate(unique_values)\n",
    "               if original is not np.nan}\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def label_encode_columns(df, cat_cols, mappings):\n",
    "    \"\"\"\n",
    "    Label encode a dataframe categorical columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with columns to be encoded.\n",
    "    cat_cols: list of str\n",
    "        Column names to be encoded.\n",
    "    mappings: dict of dict\n",
    "        Dictionary containing a key-value mapping for each column to be\n",
    "        encoded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with the encoded columns added and the `cat_cols` removed.\n",
    "    encoded_col_names: list of str\n",
    "        Encoded column names.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    build_encoding_map : Build a series encoding mapping.\n",
    "\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    encoded_col_names = []\n",
    "    for col in cat_cols:\n",
    "        new_col = f'{col}_encoded'\n",
    "        encoded_col_names.append(new_col)\n",
    "\n",
    "        df[new_col] = df[col].map(mappings[col])\n",
    "\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    return df, encoded_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5S-CRBbGc02"
   },
   "source": [
    "# Target maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1612482610488,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "pKPoKfl6Geev"
   },
   "outputs": [],
   "source": [
    "KEYS_TO_ORDINAL = {\n",
    "    30000: 0,\n",
    "    65030: 1,\n",
    "    65000: 2,\n",
    "    80000: 3,\n",
    "    74000: 4,\n",
    "    70000: 5,\n",
    "    70032: 6,\n",
    "    88000: 7,\n",
    "    86000: 8,\n",
    "    99000: 9,\n",
    "    90000: 10,\n",
    "    93000: 11\n",
    "    }\n",
    "\n",
    "\n",
    "KEYS_TO_LITHOLOGY = {30000: 'Sandstone',\n",
    "                     65030: 'Sandstone/Shale',\n",
    "                     65000: 'Shale',\n",
    "                     80000: 'Marl',\n",
    "                     74000: 'Dolomite',\n",
    "                     70000: 'Limestone',\n",
    "                     70032: 'Chalk',\n",
    "                     88000: 'Halite',\n",
    "                     86000: 'Anhydrite',\n",
    "                     99000: 'Tuff',\n",
    "                     90000: 'Coal',\n",
    "                     93000: 'Basement'}\n",
    "\n",
    "ORDINAL_TO_KEYS = {value: key for key, value in  KEYS_TO_ORDINAL.items()}\n",
    "\n",
    "ORDINAL_TO_LITHOLOGY = {}\n",
    "for ordinal_key, key in ORDINAL_TO_KEYS.items():\n",
    "    ORDINAL_TO_LITHOLOGY[ordinal_key] = KEYS_TO_LITHOLOGY[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-_H1SeJu1ho"
   },
   "source": [
    "# Import data\n",
    "\n",
    "First add a shortcut from the [google drive competition data location](https://drive.google.com/drive/folders/1GIkjq4fwgwbiqVQxYwoJnOJWVobZ91pL) to your own google drive. We will mount this drive, and access the data from it.\n",
    "\n",
    "We will save the results to a diffent folder, where we have write access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1612482612311,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "fEYM1Xz2u1ht",
    "outputId": "4db6a32d-b1d1-4c7a-a59d-a139759d246f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1612482612956,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "RpIq9zscu1hu"
   },
   "outputs": [],
   "source": [
    "#should be edited to the present working directory of the user\n",
    "data_source = '/content/drive/My Drive/FORCE 2020 lithofacies prediction from well logs competition/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 3664,
     "status": "ok",
     "timestamp": 1612482928460,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "7BG8lQNYu1hu"
   },
   "outputs": [],
   "source": [
    "penalty_matrix = np.load(data_source + 'penalty_matrix.npy')\n",
    "\n",
    "train = pd.read_csv(data_source + 'CSV_train.csv', sep=';')\n",
    "\n",
    "test = pd.read_csv(data_source + 'CSV_test.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1612482929254,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "dt2Oge4Fu1hv"
   },
   "outputs": [],
   "source": [
    "# Destination folder\n",
    "out_data_dir = Path('/content/drive/My Drive/lith_pred/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJyhGrd8PDrK"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1612482930548,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "AmSp3xSsu1hv"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    '''\n",
    "    class to lithology prediction\n",
    "    '''\n",
    "    def preprocess(self, df, cat_columns, mappings):\n",
    "\n",
    "        # # Drop model features\n",
    "        # drop_cols = [\n",
    "        #              'FORCE_2020_LITHOFACIES_CONFIDENCE',\n",
    "        #              'SGR',\n",
    "        #              'DTS',\n",
    "        #              'DCAL',\n",
    "        #              'RMIC',\n",
    "        #              'ROPA',\n",
    "        #              'RXO',                     \n",
    "        #              ]\n",
    "\n",
    "        # # Confirm drop columns are in df\n",
    "        # drop_cols = [col for col in drop_cols if col in df.columns]\n",
    "\n",
    "        # df.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "        # Label encode\n",
    "        df, encoded_col_names = label_encode_columns(df, cat_columns, mappings)\n",
    "        \n",
    "        # Augment using Bestagini's functions\n",
    "        df_preprocesed = shift_concat_gradient(df,\n",
    "                                               'DEPTH_MD',\n",
    "                                               'WELL',\n",
    "                                               encoded_col_names,\n",
    "                                               periods=1,\n",
    "                                               fill_value=None)\n",
    "       \n",
    "        return df_preprocesed\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        split = 5\n",
    "        skf = StratifiedKFold(n_splits=split, shuffle=True)\n",
    "\n",
    "        model = XGBClassifier(n_estimators=100, max_depth=10, booster='gbtree',\n",
    "                              objective='multi:softprob', learning_rate=0.1, random_state=0,\n",
    "                              subsample=0.9, colsample_bytree=0.9, tree_method='gpu_hist',\n",
    "                              eval_metric='mlogloss', verbose=2020, reg_lambda=1500)\n",
    "        \n",
    "        models = []\n",
    "        for fold_number, indices in enumerate(skf.split(X, y)):\n",
    "            print(f'Fitting fold: {fold_number}')\n",
    "            \n",
    "            train_index, test_index = indices\n",
    "\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            model.fit(X_train,\n",
    "                      y_train,\n",
    "                      early_stopping_rounds=100,\n",
    "                      eval_set=[(X_test, y_test)],\n",
    "                      verbose=100)\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        return models\n",
    "\n",
    "    def fit_predict(self, X_train, y_train, X_pred, pred_wells, save_filename):\n",
    "        # Fit\n",
    "        models = self.fit(X_train, y_train)\n",
    "\n",
    "        # Get lithologies probabilities for each model\n",
    "        models_proba = []\n",
    "        for model_num, model in enumerate(models):\n",
    "            model_proba = model.predict_proba(X_pred)\n",
    "            model_classes = [ORDINAL_TO_LITHOLOGY[lith] for lith in model.classes_]\n",
    "\n",
    "            model_proba_df = pd.DataFrame(model_proba, columns=model_classes)\n",
    "\n",
    "            model_proba_df['MODEL'] = model_num\n",
    "\n",
    "\n",
    "            # Set sample index, well, and MD\n",
    "            pred_wells_df = pred_wells.reset_index()\n",
    "            model_proba_df['index'] = pred_wells_df['index']\n",
    "            model_proba_df['WELL'] = pred_wells_df['WELL']\n",
    "\n",
    "            md = X_pred['DEPTH_MD']\n",
    "            md.reset_index(inplace=True, drop=True)\n",
    "            model_proba_df['DEPTH_MD'] = md\n",
    "\n",
    "            models_proba.append(model_proba_df)\n",
    "\n",
    "        models_proba = pd.concat(models_proba, ignore_index=True)\n",
    "\n",
    "        # Create save directory if it doesn't exists\n",
    "        if not save_filename.parent.is_dir():\n",
    "            save_filename.parent.mkdir(parents=True)\n",
    "\n",
    "        # Save models_proba to CSV\n",
    "        models_proba.to_csv(save_filename, index=False)\n",
    "\n",
    "        return models, models_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0mFUwb2xlHl"
   },
   "source": [
    "# Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1612482931932,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "E9VZFXQgsevf"
   },
   "outputs": [],
   "source": [
    "# Build group of groups\n",
    "group_of_groups = {\n",
    "    'VIKING GP.': 'VTB GP.',\n",
    "    'BOKNFJORD GP.': 'VTB GP.',\n",
    "    'TYNE GP.': 'VTB GP.',\n",
    "    'ROTLIEGENDES GP.': 'PERMIAN GP.',\n",
    "    'ZECHSTEIN GP.': 'PERMIAN GP.',\n",
    "    }\n",
    "\n",
    "train['GROUPED'] = train['GROUP']\n",
    "train['GROUPED'].replace(group_of_groups, inplace=True)\n",
    "\n",
    "train.drop('GROUP', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1612482931932,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "1y3VnHosselz"
   },
   "outputs": [],
   "source": [
    "train['FORCE_2020_LITHOFACIES_LITHOLOGY'] = train['FORCE_2020_LITHOFACIES_LITHOLOGY'].map(KEYS_TO_ORDINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1612482932579,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "ZChXlgMwsea4"
   },
   "outputs": [],
   "source": [
    "cat_columns = ['FORMATION']\n",
    "\n",
    "train_mappings = {col: build_encoding_map(train[col]) for col in cat_columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1612482932883,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "exqiPxpTv1k_"
   },
   "outputs": [],
   "source": [
    "# Drop columns with high percent of missing values\n",
    "drop_cols = [\n",
    "            'FORCE_2020_LITHOFACIES_CONFIDENCE',\n",
    "            'SGR',\n",
    "            'DTS',\n",
    "            'DCAL',\n",
    "            'RMIC',\n",
    "            'ROPA',\n",
    "            'RXO',                     \n",
    "            ]\n",
    "\n",
    "# Confirm drop columns are in df\n",
    "drop_cols = [col for col in drop_cols if col in train.columns]\n",
    "\n",
    "train.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1612482933540,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "1IsSu_X2vRkw"
   },
   "outputs": [],
   "source": [
    "# Use different logs per group\n",
    "limit = 0.68\n",
    "keep_logs_per_group = {}\n",
    "for group_data in train.groupby('GROUPED'):\n",
    "    group_name, group = group_data\n",
    "    \n",
    "    group_log_coverage = (~group.isna()).sum() / group.shape[0]\n",
    "    \n",
    "    cond_more_than_limit = group_log_coverage > limit\n",
    "    \n",
    "    keep_logs = [log for log, val in cond_more_than_limit.items() if val]\n",
    "\n",
    "    if 'FORMATION' not in keep_logs:\n",
    "        keep_logs.append('FORMATION')\n",
    "    \n",
    "    keep_logs_per_group[group_name] = keep_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eM4FKCJOuJn"
   },
   "source": [
    "# Prepare predict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1612482933707,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "tpCF1AdcOyTu"
   },
   "outputs": [],
   "source": [
    "test['GROUPED'] = test['GROUP']\n",
    "test['GROUPED'].replace(group_of_groups, inplace=True)\n",
    "\n",
    "test.drop('GROUP', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1612482934367,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "YCHKzGqdPAaL"
   },
   "outputs": [],
   "source": [
    "# Confirm drop columns are in df\n",
    "test_drop_cols = [col for col in drop_cols if col in test.columns]\n",
    "\n",
    "test.drop(test_drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1612482934738,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "39B5rtkWPSuH",
    "outputId": "81a12bc6-e9ae-4632-b281-16c72d7008ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WELL', 'DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'FORMATION', 'CALI',\n",
       "       'RSHA', 'RMED', 'RDEP', 'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS',\n",
       "       'ROP', 'DRHO', 'MUDWEIGHT', 'GROUPED'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bi48QyIPgGj"
   },
   "source": [
    "# Fit predict groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 632336,
     "status": "ok",
     "timestamp": 1612483568019,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "BUFP5WpAuLMg",
    "outputId": "8bed9cd8-e30a-4917-bdb9-d2ec2ab0a800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting group: BAAT GP.\n",
      "Fitting fold: 0\n",
      "[0]\tvalidation_0-mlogloss:1.85096\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.576368\n",
      "Fitting fold: 1\n",
      "[0]\tvalidation_0-mlogloss:1.85075\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.568575\n",
      "Fitting fold: 2\n",
      "[0]\tvalidation_0-mlogloss:1.85051\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.575224\n",
      "Fitting fold: 3\n",
      "[0]\tvalidation_0-mlogloss:1.85116\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.583879\n",
      "Fitting fold: 4\n",
      "[0]\tvalidation_0-mlogloss:1.85122\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.582634\n",
      "Fitting group: CROMER KNOLL GP.\n",
      "Fitting fold: 0\n",
      "[0]\tvalidation_0-mlogloss:1.95655\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.454039\n",
      "Fitting fold: 1\n",
      "[0]\tvalidation_0-mlogloss:1.9561\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.445709\n",
      "Fitting fold: 2\n",
      "[0]\tvalidation_0-mlogloss:1.95587\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.435278\n",
      "Fitting fold: 3\n",
      "[0]\tvalidation_0-mlogloss:1.95665\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.452873\n",
      "Fitting fold: 4\n",
      "[0]\tvalidation_0-mlogloss:1.95603\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.44595\n",
      "Fitting group: DUNLIN GP.\n",
      "Fitting fold: 0\n",
      "[0]\tvalidation_0-mlogloss:1.7908\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.397399\n",
      "Fitting fold: 1\n",
      "[0]\tvalidation_0-mlogloss:1.7917\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.401991\n",
      "Fitting fold: 2\n",
      "[0]\tvalidation_0-mlogloss:1.79112\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.396857\n",
      "Fitting fold: 3\n",
      "[0]\tvalidation_0-mlogloss:1.79127\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.394878\n",
      "Fitting fold: 4\n",
      "[0]\tvalidation_0-mlogloss:1.79093\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.400352\n",
      "Fitting group: HEGRE GP.\n",
      "Fitting fold: 0\n",
      "[0]\tvalidation_0-mlogloss:1.73131\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.557748\n",
      "Fitting fold: 1\n",
      "[0]\tvalidation_0-mlogloss:1.73177\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.561459\n",
      "Fitting fold: 2\n",
      "[0]\tvalidation_0-mlogloss:1.73064\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.542982\n",
      "Fitting fold: 3\n",
      "[0]\tvalidation_0-mlogloss:1.73179\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.558272\n",
      "Fitting fold: 4\n",
      "[0]\tvalidation_0-mlogloss:1.73163\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.560911\n",
      "Fitting group: HORDALAND GP.\n",
      "Fitting fold: 0\n",
      "[0]\tvalidation_0-mlogloss:1.70809\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.19608\n",
      "Fitting fold: 1\n",
      "[0]\tvalidation_0-mlogloss:1.70782\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.192966\n",
      "Fitting fold: 2\n",
      "[0]\tvalidation_0-mlogloss:1.70773\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.192667\n",
      "Fitting fold: 3\n",
      "[0]\tvalidation_0-mlogloss:1.70809\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.194006\n",
      "Fitting fold: 4\n",
      "[0]\tvalidation_0-mlogloss:1.708\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.196331\n",
      "Fitting group: NORDLAND GP.\n",
      "Fitting fold: 0\n",
      "[0]\tvalidation_0-mlogloss:1.62616\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.241628\n",
      "Fitting fold: 1\n",
      "[0]\tvalidation_0-mlogloss:1.62576\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.235304\n",
      "Fitting fold: 2\n",
      "[0]\tvalidation_0-mlogloss:1.62599\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.240096\n",
      "Fitting fold: 3\n",
      "[0]\tvalidation_0-mlogloss:1.62534\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.237415\n",
      "Fitting fold: 4\n",
      "[0]\tvalidation_0-mlogloss:1.62624\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.240117\n",
      "Fitting group: PERMIAN GP.\n",
      "Fitting fold: 0\n",
      "[0]\tvalidation_0-mlogloss:2.06686\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.342774\n",
      "Fitting fold: 1\n",
      "[0]\tvalidation_0-mlogloss:2.06698\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.336025\n",
      "Fitting fold: 2\n",
      "[0]\tvalidation_0-mlogloss:2.06765\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.349392\n",
      "Fitting fold: 3\n",
      "[0]\tvalidation_0-mlogloss:2.06719\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.357769\n",
      "Fitting fold: 4\n",
      "[0]\tvalidation_0-mlogloss:2.06744\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.364971\n",
      "Fitting group: ROGALAND GP.\n",
      "Fitting fold: 0\n",
      "[0]\tvalidation_0-mlogloss:1.77599\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.327947\n",
      "Fitting fold: 1\n",
      "[0]\tvalidation_0-mlogloss:1.77644\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.327094\n",
      "Fitting fold: 2\n",
      "[0]\tvalidation_0-mlogloss:1.77647\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.327863\n",
      "Fitting fold: 3\n",
      "[0]\tvalidation_0-mlogloss:1.77544\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.326039\n",
      "Fitting fold: 4\n",
      "[0]\tvalidation_0-mlogloss:1.77611\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.324349\n",
      "Fitting group: SHETLAND GP.\n",
      "Fitting fold: 0\n",
      "[0]\tvalidation_0-mlogloss:1.73954\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.266251\n",
      "Fitting fold: 1\n",
      "[0]\tvalidation_0-mlogloss:1.73935\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.261774\n",
      "Fitting fold: 2\n",
      "[0]\tvalidation_0-mlogloss:1.73897\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.26587\n",
      "Fitting fold: 3\n",
      "[0]\tvalidation_0-mlogloss:1.73914\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.26276\n",
      "Fitting fold: 4\n",
      "[0]\tvalidation_0-mlogloss:1.73936\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.263598\n",
      "Fitting group: VESTLAND GP.\n",
      "Fitting fold: 0\n",
      "[0]\tvalidation_0-mlogloss:1.68775\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.373336\n",
      "Fitting fold: 1\n",
      "[0]\tvalidation_0-mlogloss:1.6888\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.390879\n",
      "Fitting fold: 2\n",
      "[0]\tvalidation_0-mlogloss:1.68851\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.382111\n",
      "Fitting fold: 3\n",
      "[0]\tvalidation_0-mlogloss:1.68817\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.384852\n",
      "Fitting fold: 4\n",
      "[0]\tvalidation_0-mlogloss:1.68962\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.391549\n",
      "Fitting group: VTB GP.\n",
      "Fitting fold: 0\n",
      "[0]\tvalidation_0-mlogloss:1.90474\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.359042\n",
      "Fitting fold: 1\n",
      "[0]\tvalidation_0-mlogloss:1.90166\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.362713\n",
      "Fitting fold: 2\n",
      "[0]\tvalidation_0-mlogloss:1.9013\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.354426\n",
      "Fitting fold: 3\n",
      "[0]\tvalidation_0-mlogloss:1.90365\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.357954\n",
      "Fitting fold: 4\n",
      "[0]\tvalidation_0-mlogloss:1.90455\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.360191\n"
     ]
    }
   ],
   "source": [
    "for group_data in test.groupby('GROUPED'):\n",
    "    group_name, group = group_data\n",
    "\n",
    "    if group_name in train['GROUPED'].unique():\n",
    "        # Select train group features\n",
    "        keep_cols = keep_logs_per_group[group_name]\n",
    "        group_train = train.loc[train['GROUPED']==group_name, keep_cols]\n",
    "        group_train.drop(['GROUPED'], axis=1, inplace=True)\n",
    "\n",
    "        # Drop lithofacies with less than n_split samples\n",
    "        train_group_vc = group_train['FORCE_2020_LITHOFACIES_LITHOLOGY'].value_counts()\n",
    "\n",
    "        for lith, count in train_group_vc.items():\n",
    "            if count <= 5:\n",
    "                cond = group_train['FORCE_2020_LITHOFACIES_LITHOLOGY'] != lith\n",
    "                group_train = group_train.loc[cond, :]\n",
    "\n",
    "        model = Model()\n",
    "\n",
    "        # Define train target and features\n",
    "        y_train = group_train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "    \n",
    "        X = group_train.drop('FORCE_2020_LITHOFACIES_LITHOLOGY', axis=1)\n",
    "        \n",
    "        # Augment train features\n",
    "        X_train = model.preprocess(X, cat_columns, train_mappings)\n",
    "\n",
    "        X_train.drop('WELL', axis=1, inplace=True)\n",
    "\n",
    "        # Define predict features\n",
    "        pred_keep_cols = [col for col in keep_cols if col != 'FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "        X_pred = group.loc[:, pred_keep_cols]\n",
    "        X_pred.drop(['GROUPED'], axis=1, inplace=True)\n",
    "        \n",
    "        # Augment predict features\n",
    "        X_pred = model.preprocess(X_pred, cat_columns, train_mappings)\n",
    "\n",
    "        X_pred.drop('WELL', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        fn = '_'.join(group_name.lower().split())\n",
    "        save_filename = out_data_dir / f'model_proba/grouped/00/models_proba_grouped_{fn}csv'\n",
    "\n",
    "        predict_group_wells = group['WELL']\n",
    "\n",
    "        print(f'Fitting group: {group_name}')\n",
    "        models, models_proba = model.fit_predict(X_train,\n",
    "                                                 y_train,\n",
    "                                                 X_pred,\n",
    "                                                 predict_group_wells,\n",
    "                                                 save_filename)\n",
    "        # print(X_pred.shape)\n",
    "        # print()\n",
    "        # print(predict_group_wells.shape)\n",
    "        # print(predict_group_wells.head())\n",
    "        # print(predict_group_wells.tail())\n",
    "        # print()\n",
    "\n",
    "\n",
    "    else:\n",
    "        print('Group {group_name} is in the prediction set')\n",
    "        print('but not in the train set')\n",
    "        print('This functionality is currently not supported')\n",
    "        print()\n",
    "\n",
    "        # TODO: What happens when there is a group in the predict set but not in the train set?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "9.0-rp-fit-predict-save-proba-grouped-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
