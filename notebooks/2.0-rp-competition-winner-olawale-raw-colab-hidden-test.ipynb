{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1238,
     "status": "ok",
     "timestamp": 1610919769060,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "U4wK_1Sp5oZz"
   },
   "outputs": [],
   "source": [
    "#importing required libraries and packages\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1610919770314,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "ff7CdOKkPise",
    "outputId": "789b131b-dbf3-4eb7-bd50-178356daaeef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version is: 1.1.5\n",
      "numpy version is: 1.19.5\n",
      "matplotlib version is: 3.2.2\n",
      "sklearn version is: 0.22.2.post1\n",
      "xgboost version is: 0.90\n"
     ]
    }
   ],
   "source": [
    "#printing out versions of all packages and libraries and used\n",
    "\n",
    "print(f'pandas version is: {pd.__version__}')\n",
    "print(f'numpy version is: {np.__version__}')\n",
    "print(f'matplotlib version is: {matplotlib.__version__}')\n",
    "print(f'sklearn version is: {sklearn.__version__}')\n",
    "print(f'xgboost version is: {xgb.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1610919771290,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "Z1HHu00J6gBZ"
   },
   "outputs": [],
   "source": [
    "#all helper functions used\n",
    "\n",
    "def drop_columns(data, *args):\n",
    "\n",
    "    '''\n",
    "    function used to drop columns.\n",
    "    args:: \n",
    "      data:  dataframe to be operated on\n",
    "      *args: a list of columns to be dropped from the dataframe\n",
    "\n",
    "    return: returns a dataframe with the columns dropped\n",
    "    '''\n",
    "    \n",
    "    columns = []\n",
    "    for _ in args:\n",
    "        columns.append(_)\n",
    "        \n",
    "    data = data.drop(columns, axis=1)\n",
    "        \n",
    "    return data\n",
    " \n",
    "def process(data):\n",
    "\n",
    "    '''\n",
    "    function to process dataframe by replacing missing, infinity values with -999\n",
    "\n",
    "    args:: \n",
    "      data:  dataframe to be operated on\n",
    "    \n",
    "    returns dataframe with replaced values\n",
    "    '''\n",
    "    \n",
    "    cols = list(data.columns)\n",
    "    for _ in cols:\n",
    "\n",
    "        data[_] = np.where(data[_] == np.inf, -999, data[_])\n",
    "        data[_] = np.where(data[_] == np.nan, -999, data[_])\n",
    "        data[_] = np.where(data[_] == -np.inf, -999, data[_])\n",
    "        \n",
    "    return data\n",
    " \n",
    "def show_evaluation(pred, true):\n",
    "\n",
    "  '''\n",
    "\n",
    "  function to show model performance and evaluation\n",
    "  args:\n",
    "    pred: predicted value(a list)\n",
    "    true: actual values (a list)\n",
    "\n",
    "  prints the custom metric performance, accuracy and F1 score of predictions\n",
    "\n",
    "  '''\n",
    "\n",
    "  print(f'Default score: {score(true.values, pred)}')\n",
    "  print(f'Accuracy is: {accuracy_score(true, pred)}')\n",
    "  print(f'F1 is: {f1_score(pred, true.values, average=\"weighted\")}')\n",
    "\n",
    "\n",
    "#Paulo Bestagini's feature augmentation technique from SEG 2016 ML competition\n",
    "#Link : https://github.com/seg/2016-ml-contest/tree/master/ispl\n",
    "\n",
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    " \n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    " \n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    " \n",
    "    return X_aug\n",
    " \n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    " \n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "    \n",
    "    return X_aug, padded_rows\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "\n",
    "    '''\n",
    "    custom metric used for evaluation\n",
    "    args:\n",
    "      y_true: actual prediction\n",
    "      y_pred: predictions made\n",
    "    '''\n",
    "\n",
    "    S = 0.0\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    for i in range(0, y_true.shape[0]):\n",
    "        S -= A[y_true[i], y_pred[i]]\n",
    "    return S/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19018,
     "status": "ok",
     "timestamp": 1610919791526,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "SMZi5ywWUCQV",
    "outputId": "b06f75d8-6201-4181-8a78-a59c9b3d5de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1610919795192,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "kSQfmlIWxBM8"
   },
   "outputs": [],
   "source": [
    "#should be edited to the present working directory of the user\n",
    "PWD = '/content/drive/My Drive/FORCE 2020 lithofacies prediction from well logs competition/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6125,
     "status": "ok",
     "timestamp": 1610919802038,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "1cLS1yO56iT_"
   },
   "outputs": [],
   "source": [
    "#importing penaltry matrix used for evaluation and train and test files\n",
    "A = np.load(PWD + 'penalty_matrix.npy')\n",
    "\n",
    "train = pd.read_csv(PWD + 'CSV_train.csv', sep=';')\n",
    "\n",
    "test = pd.read_csv(PWD + 'CSV_hidden_test.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1610919805455,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "xQSZM0glXhPD"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "\n",
    "    '''\n",
    "    class to lithology prediction\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, train, test):\n",
    "\n",
    "        '''\n",
    "        takes in the train and test dataframes\n",
    "        '''\n",
    "        \n",
    "        self.train = train\n",
    "        self.test = test\n",
    "\n",
    "        \n",
    "    def __call__(self, plot = True):\n",
    "\n",
    "      return self.fit(plot)\n",
    "\n",
    "    def preprocess(self, train, test):\n",
    "\n",
    "        '''\n",
    "        method to prepare datasets for training and predictions\n",
    "        accepts both the train and test dataframes as arguments\n",
    "\n",
    "        returns the prepared train, test datasets along with the\n",
    "        lithology labels and numbers which is needed for preparing\n",
    "        the submission file\n",
    "\n",
    "        '''\n",
    "\n",
    "        #concatenating both train and test datasets for easier and uniform processing\n",
    "\n",
    "        ntrain = train.shape[0]\n",
    "        ntest = test.shape[0]\n",
    "        target = train.FORCE_2020_LITHOFACIES_LITHOLOGY.copy()\n",
    "        df = pd.concat((train, test)).reset_index(drop=True)\n",
    "\n",
    "        #mapping the lithology labels to ordinal values for better modelling\n",
    "\n",
    "        lithology = train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    " \n",
    "        lithology_numbers = {30000: 0,\n",
    "                        65030: 1,\n",
    "                        65000: 2,\n",
    "                        80000: 3,\n",
    "                        74000: 4,\n",
    "                        70000: 5,\n",
    "                        70032: 6,\n",
    "                        88000: 7,\n",
    "                        86000: 8,\n",
    "                        99000: 9,\n",
    "                        90000: 10,\n",
    "                        93000: 11}\n",
    "        \n",
    "        lithology1 = lithology.map(lithology_numbers)\n",
    "\n",
    "        #implementing Bestagini's augmentation procedure\n",
    "\n",
    "        train_well = train.WELL.values\n",
    "        train_depth = train.DEPTH_MD.values\n",
    "        \n",
    "        test_well = test.WELL.values\n",
    "        test_depth = test.DEPTH_MD.values  \n",
    "        '''to be continued...\n",
    "        #this was done here for ease as the datasets would undergo some transformations\n",
    "        #that would make it uneasy to perform the augmentation technique'''\n",
    "\n",
    "        \n",
    "\n",
    "        print(f'shape of concatenated dataframe before dropping columns {df.shape}')\n",
    "\n",
    "        cols = ['FORCE_2020_LITHOFACIES_CONFIDENCE', 'SGR', 'DTS', 'RXO', 'ROPA'] #columns to be dropped\n",
    "        df = drop_columns(df, *cols)\n",
    "        print(f'shape of dataframe after dropping columns {df.shape}')\n",
    "        print(f'{cols} were dropped')\n",
    "\n",
    "        #Label encoding the GROUP, FORMATION and WELLS features as these improved the performance of the models on validations\n",
    "\n",
    "        df['GROUP_encoded'] = df['GROUP'].astype('category')\n",
    "        df['GROUP_encoded'] = df['GROUP_encoded'].cat.codes \n",
    "        df['FORMATION_encoded'] = df['FORMATION'].astype('category')\n",
    "        df['FORMATION_encoded'] = df['FORMATION_encoded'].cat.codes\n",
    "        df['WELL_encoded'] = df['WELL'].astype('category')\n",
    "        df['WELL_encoded'] = df['WELL_encoded'].cat.codes\n",
    "        print(f'shape of dataframe after label encoding columns {df.shape}')\n",
    "\n",
    "\n",
    "        #FURTHER PREPRATION TO SPLIT DATAFRAME INTO TRAIN AND TEST DATASETS AFTER PREPRATION\n",
    "        print(f'Splitting concatenated dataframe into training and test datasets...')\n",
    "        df = df.drop(['WELL', 'GROUP', 'FORMATION'], axis=1)\n",
    "        print(df.shape)\n",
    "        \n",
    "        df = df.fillna(-999)\n",
    "        df = process(df)\n",
    "        data = df.copy()\n",
    "        \n",
    "        train2 = data[:ntrain].copy()\n",
    "        train2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "        \n",
    "        test2 = data[ntrain:(ntest+ntrain)].copy()\n",
    "        test2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "        test2 = test2.reset_index(drop=True)\n",
    "\n",
    "        traindata = train2\n",
    "        testdata = test2\n",
    "\n",
    "        print(f'Shape of train and test datasets before augmentation {traindata.shape, testdata.shape}')\n",
    " \n",
    "        traindata1, padded_rows = augment_features(pd.DataFrame(traindata).values, train_well, train_depth)\n",
    "        testdata1, padded_rows = augment_features(pd.DataFrame(testdata).values, test_well, test_depth)\n",
    "        \n",
    "        print(f'Shape of train and test datasets after augmentation {traindata1.shape, testdata1.shape}')\n",
    "    \n",
    "        return traindata1, testdata1, lithology1, lithology_numbers\n",
    "\n",
    "    \n",
    "    def fit(self, plot):\n",
    "\n",
    "      '''\n",
    "      method to train model and make predictions\n",
    "\n",
    "      returns the test predictions, trained model, and lithology numbers used for making the submission file\n",
    "      '''\n",
    "\n",
    "      traindata1, testdata1, lithology1, lithology_numbers = self.preprocess(self.train, self.test)\n",
    "\n",
    "      #using a 10-fold stratified cross-validation technique and seting the shuffle parameter to true\n",
    "      #as this improved the validation performance better\n",
    "\n",
    "      split = 10\n",
    "      kf = StratifiedKFold(n_splits=split, shuffle=True)\n",
    "  \n",
    "      open_test = np.zeros((len(testdata1), 12))\n",
    "      \n",
    "      #100 n-estimators and 10 max-depth\n",
    "      model = XGBClassifier(n_estimators=100, max_depth=10, booster='gbtree',\n",
    "                            objective='multi:softprob', learning_rate=0.1, random_state=0,\n",
    "                            subsample=0.9, colsample_bytree=0.9, tree_method='gpu_hist',\n",
    "                            eval_metric='mlogloss', verbose=2020, reg_lambda=1500)\n",
    "      \n",
    " \n",
    "      i = 1\n",
    "      for (train_index, test_index) in kf.split(pd.DataFrame(traindata1), pd.DataFrame(lithology1)):\n",
    "        X_train, X_test = pd.DataFrame(traindata1).iloc[train_index], pd.DataFrame(traindata1).iloc[test_index]\n",
    "        Y_train, Y_test = pd.DataFrame(lithology1).iloc[train_index],pd.DataFrame(lithology1).iloc[test_index]\n",
    "    \n",
    "        model.fit(X_train, Y_train, early_stopping_rounds=100, eval_set=[(X_test, Y_test)], verbose=100)\n",
    "        prediction = model.predict(X_test)\n",
    "        print(show_evaluation(prediction, Y_test))\n",
    " \n",
    "        print(f'-----------------------FOLD {i}---------------------')\n",
    "        i+=1\n",
    " \n",
    "        open_test += model.predict_proba(pd.DataFrame(testdata1))\n",
    "      \n",
    "      open_test= pd.DataFrame(open_test/split)\n",
    "    \n",
    "      open_test = np.array(pd.DataFrame(open_test).idxmax(axis=1))\n",
    " \n",
    "      print('---------------CROSS VALIDATION COMPLETE')\n",
    "      print('----------------TEST EVALUATION------------------')\n",
    "\n",
    "                  \n",
    "      if plot: self.plot_feat_imp(model)\n",
    "      return open_test, model, lithology_numbers\n",
    "              \n",
    "              \n",
    "    def plot_feat_imp(self, model):\n",
    "        feat_imp = pd.Series(model.feature_importances_).sort_values(ascending=False)\n",
    "        plt.figure(figsize=(12,8))\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "\n",
    "    def make_submission_file(self, filename):\n",
    "\n",
    "      '''\n",
    "      method to train model, make prediction and create submission file\n",
    "      args::\n",
    "        filename: name to save submission file as (string)\n",
    "      '''\n",
    "\n",
    "      self.filename = filename\n",
    "\n",
    "      prediction, model, lithology_numbers = self.fit(plot=False)\n",
    " \n",
    "      path = PWD\n",
    "      # Destination folder\n",
    "      out_data_dir = '/content/drive/My Drive/lith_pred/'\n",
    "\n",
    "    \n",
    "      test = pd.read_csv(PWD + 'CSV_test.csv', sep=';')\n",
    "      \n",
    "      category_to_lithology = {y:x for x,y in lithology_numbers.items()}\n",
    "      test_prediction_for_submission = np.vectorize(category_to_lithology.get)(prediction)\n",
    "      np.savetxt(out_data_dir+filename+'.csv', test_prediction_for_submission, header='lithology', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1610919816544,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "Fd45KJLVk3Ka"
   },
   "outputs": [],
   "source": [
    "#initializing the model class\n",
    "\n",
    "func_= Model(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 806934,
     "status": "ok",
     "timestamp": 1610920626234,
     "user": {
      "displayName": "Rafael Pinto",
      "photoUrl": "",
      "userId": "01213912175113627450"
     },
     "user_tz": 360
    },
    "id": "zRywwBE3mCb7",
    "outputId": "2aab5285-baa9-4053-8f25-603cc689c2f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of concatenated dataframe before dropping columns (1292908, 29)\n",
      "shape of dataframe after dropping columns (1292908, 24)\n",
      "['FORCE_2020_LITHOFACIES_CONFIDENCE', 'SGR', 'DTS', 'RXO', 'ROPA'] were dropped\n",
      "shape of dataframe after label encoding columns (1292908, 27)\n",
      "Splitting concatenated dataframe into training and test datasets...\n",
      "(1292908, 24)\n",
      "Shape of train and test datasets before augmentation ((1170511, 23), (122397, 23))\n",
      "Shape of train and test datasets after augmentation ((1170511, 92), (122397, 92))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.16441\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.319967\n",
      "Default score: [-0.27769389]\n",
      "Accuracy is: 0.8940898062399617\n",
      "F1 is: 0.8979671517204558\n",
      "None\n",
      "-----------------------FOLD 1---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16438\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.319631\n",
      "Default score: [-0.2788496]\n",
      "Accuracy is: 0.893866776020709\n",
      "F1 is: 0.8977725875817484\n",
      "None\n",
      "-----------------------FOLD 2---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16453\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.317647\n",
      "Default score: [-0.27892649]\n",
      "Accuracy is: 0.8943537432401261\n",
      "F1 is: 0.8983127271242703\n",
      "None\n",
      "-----------------------FOLD 3---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16504\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.319138\n",
      "Default score: [-0.27706406]\n",
      "Accuracy is: 0.8944050029474332\n",
      "F1 is: 0.8983297303769758\n",
      "None\n",
      "-----------------------FOLD 4---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16435\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.319874\n",
      "Default score: [-0.27824196]\n",
      "Accuracy is: 0.894012011858079\n",
      "F1 is: 0.897949759006275\n",
      "None\n",
      "-----------------------FOLD 5---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16458\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.320841\n",
      "Default score: [-0.2807067]\n",
      "Accuracy is: 0.893490871500457\n",
      "F1 is: 0.8974605949925181\n",
      "None\n",
      "-----------------------FOLD 6---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16381\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.320404\n",
      "Default score: [-0.27716444]\n",
      "Accuracy is: 0.8946613014839685\n",
      "F1 is: 0.89851685616484\n",
      "None\n",
      "-----------------------FOLD 7---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16429\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.319843\n",
      "Default score: [-0.27956938]\n",
      "Accuracy is: 0.8939778387198742\n",
      "F1 is: 0.898024429484239\n",
      "None\n",
      "-----------------------FOLD 8---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16365\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.319295\n",
      "Default score: [-0.2785869]\n",
      "Accuracy is: 0.8944391760856378\n",
      "F1 is: 0.8984084764306457\n",
      "None\n",
      "-----------------------FOLD 9---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16436\n",
      "Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-mlogloss:0.318868\n",
      "Default score: [-0.27636137]\n",
      "Accuracy is: 0.894874883597748\n",
      "F1 is: 0.8988982868130119\n",
      "None\n",
      "-----------------------FOLD 10---------------------\n",
      "---------------CROSS VALIDATION COMPLETE\n",
      "----------------TEST EVALUATION------------------\n"
     ]
    }
   ],
   "source": [
    "#using the make_submission_file method to make predicction and create a submission file\n",
    "\n",
    "func_.make_submission_file(filename='hidden_test_1')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2.0-rp-competition-winner-olawale-raw.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/bolgebrygg/Force-2020-Machine-Learning-competition/blob/master/lithology_competition/code/OlawaleI/FORCE_Submission_File.ipynb",
     "timestamp": 1610915647976
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
