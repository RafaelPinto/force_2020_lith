{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"9.2-rp-fit-predict-save-proba-grouped-hidden-colab.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"cells":[{"cell_type":"code","metadata":{"id":"I2sl9W3Au1hZ"},"source":["from pathlib import Path\n","import pickle\n","\n","import numpy as np\n","import numpy.random as nr\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","import pandas as pd\n","\n","import sklearn\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import preprocessing\n","import sklearn.model_selection as ms\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","import xgboost as xgb\n","from xgboost import XGBClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QyMvK7e8u1hg","executionInfo":{"status":"ok","timestamp":1612959738642,"user_tz":360,"elapsed":444,"user":{"displayName":"Rafael Pinto","photoUrl":"","userId":"01213912175113627450"}},"outputId":"01663bf8-9fdf-449c-85da-9586109a41fe"},"source":["# Print out packages versions\n","print(f'pandas version is: {pd.__version__}')\n","print(f'numpy version is: {np.__version__}')\n","print(f'matplotlib version is: {matplotlib.__version__}')\n","print(f'sklearn version is: {sklearn.__version__}')\n","print(f'xgboost version is: {xgb.__version__}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["pandas version is: 1.1.5\n","numpy version is: 1.19.5\n","matplotlib version is: 3.2.2\n","sklearn version is: 0.22.2.post1\n","xgboost version is: 0.90\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HE9_Q2bsu1hi"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"m1x1Oo6ju1hi"},"source":["def replace_nan_inf(df, value=None):\n","    \"\"\"\n","    Replace missing and infinity values.\n","\n","    Parameters\n","    ----------\n","    df : pandas.DataFrame\n","        Dataframe with values to be replaced.\n","\n","    value : int, float\n","        Value to replace any missing or numpy.inf values. Defaults to numpy.nan\n","    Returns\n","    -------\n","    pandas.DataFrame\n","        Dataframe with missing and infinity values replaced with -999.\n","\n","    \"\"\"\n","    if value is None:\n","        value = np.nan\n","    return df.replace(to_replace=[np.nan, np.inf, -np.inf],\n","                      value=value)\n","\n","\n","def shift_concat(df, periods=1, fill_value=None):\n","    \"\"\"\n","    Build dataframe of shifted index.\n","\n","    Parameters\n","    ----------\n","    df : pandas.DataFrame\n","        Dataframe with columns to be shifted.\n","    periods : int\n","        Number of periods to shift. Should be positive.\n","    fill_value : object, optional\n","        The scalar value to use for newly introduced missing values. Defaults\n","        to numpy.nan.\n","\n","    Returns\n","    -------\n","    pandas.DataFrame\n","        Shifted dataframes concatenated along columns axis.\n","\n","    Notes\n","    -------\n","    Based on Paulo Bestagini's augment_features_window from SEG 2016 ML\n","    competition.\n","    https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try01.ipynb\n","\n","    Example\n","    -------\n","    Shift df by one period and concatenate.\n","\n","    >>> df = pd.DataFrame({'gr': [1.1, 2.1], 'den': [2.1, 2.2]})\n","    >>> shift_concat(df)\n","        gr_shifted_1  den_shifted_1  gr   den  gr_shifted_-1   den_shifted_-1\n","    0      NaN            NaN        1.1  2.1      2.1             2.2\n","    1      1.1            2.1        2.1  2.2      NaN             NaN\n","\n","    \"\"\"\n","    if fill_value is None:\n","        fill_value = np.nan\n","\n","    dfs = []\n","    for period in range(periods, -1*periods - 1, -1):\n","\n","        if period == 0:\n","            dfs.append(df)\n","            continue\n","\n","        df_shifted = df.shift(period, fill_value=fill_value)\n","\n","        df_shifted.columns = [f'{col}_shifted_{str(period)}'\n","                              for col in df_shifted.columns]\n","\n","        dfs.append(df_shifted)\n","\n","    return pd.concat(dfs, axis=1)\n","\n","\n","def gradient(df, depth_col):\n","    \"\"\"\n","    Calculate the gradient for all features along the provided `depth_col`\n","    column.\n","\n","    Parameters\n","    ----------\n","    df : pandas.DataFrame\n","        Dataframe with columns to be used in the gradient calculation.\n","    depth_col : str\n","        Dataframe column name to be used as depth reference.\n","\n","    Returns\n","    -------\n","    pandas.DataFrame\n","        Gradient of `df` along `depth_col` column. The depth column is not in\n","        the output dataframe.\n","\n","    Notes\n","    -------\n","    Based on Paulo Bestagini's augment_features_window from SEG 2016 ML\n","    competition.\n","    https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try01.ipynb\n","\n","    Example\n","    -------\n","    Calculate gradient of columns along `md`.\n","\n","    >>> df = pd.DataFrame({'gr': [100.1, 100.2, 100.3],\n","                          'den': [2.1, 2.2, 2.3],\n","                          'md': [500, 500.5, 501]})\n","    >>> gradient(df, 'md')\n","        gr  den\n","    0  NaN  NaN\n","    1  0.2  0.2\n","    2  0.2  0.2\n","\n","    \"\"\"\n","    depth_diff = df[depth_col].diff()\n","\n","    denom_zeros = np.isclose(depth_diff, 0)\n","    depth_diff[denom_zeros] = 0.001\n","\n","    df_diff = df.drop(depth_col, axis=1)\n","    df_diff = df_diff.diff()\n","\n","    # Add suffix to column names\n","    df_diff.columns = [f'{col}_gradient' for col in df_diff.columns]\n","\n","    return df_diff.divide(depth_diff, axis=0)\n","\n","\n","def shift_concat_gradient(df, depth_col, well_col, cat_cols, periods=1, fill_value=None):\n","    \"\"\"\n","    Augment features using `shif_concat` and `gradient`.\n","\n","    Parameters\n","    ----------\n","    df : pandas.DataFrame\n","        Dataframe with columns to be augmented.\n","    depth_col : str\n","        Dataframe column name to be used as depth reference.\n","    well_col : str\n","        Dataframe column name to be used as well reference.\n","    cat_cols: list of str\n","        Encoded column names. The gradient calculation is not applied to these\n","        columns.\n","    periods : int\n","        Number of periods to shift. Should be positive.\n","    fill_value : object, optional\n","        The scalar value to use for newly introduced missing values. Defaults\n","        to numpy.nan.\n","\n","    Returns\n","    -------\n","    pandas.DataFrame\n","        Augmented dataframe.\n","\n","    Notes\n","    -------\n","    Based on Paulo Bestagini's augment_features_window from SEG 2016 ML\n","    competition.\n","    https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try01.ipynb\n","\n","    Example\n","    -------\n","    Augment features of `df` by shifting and taking the gradient.\n","\n","    >>> df = pd.DataFrame({'gr': [100.1, 100.2, 100.3, 20.1, 20.2, 20.3],\n","                          'den': [2.1, 2.2, 2.3, 1.7, 1.8, 1.9],\n","                           'md': [500, 500.5, 501, 1000, 1000.05, 1001],\n","                         'well': [1, 1, 1, 2, 2, 2]})\n","    >>> shift_concat_gradient(df, 'md', 'well', periods=1, fill_value=None)\n","        gr_shifted_1  den_shifted_1     gr    den  ...  well   md    gr_gradient  den_gradient\n","    0         NaN          NaN         100.1  2.1  ...   1   500.00        NaN           NaN\n","    1       100.1          2.1         100.2  2.2  ...   1   500.50   0.200000      0.200000\n","    2       100.2          2.2         100.3  2.3  ...   1   501.00   0.200000      0.200000\n","    3         NaN          NaN          20.1  1.7  ...   2  1000.00        NaN           NaN\n","    4        20.1          1.7          20.2  1.8  ...   2  1000.05   2.000000      2.000000\n","    5        20.2          1.8          20.3  1.9  ...   2  1001.00   0.105263      0.105263\n","\n","    \"\"\"\n","    # TODO 'Consider filling missing values created here with DataFrame.fillna'\n","\n","    # Columns to apply gradient operation\n","    cat_cols.append(well_col)\n","    gradient_cols = [col for col in df.columns if col not in cat_cols]\n","\n","    # Don't shift depth\n","    depth = df.loc[:, depth_col]\n","\n","    grouped = df.groupby(well_col, sort=False)\n","\n","    df_aug_groups = []\n","    for name, group in grouped:\n","        shift_cols_df = group.drop([well_col, depth_col], axis=1)\n","\n","        group_shift = shift_concat(shift_cols_df,\n","                                   periods=periods,\n","                                   fill_value=fill_value)\n","\n","        # Add back the well name and depth\n","        group_shift[well_col] = name\n","        group_shift[depth_col] = depth\n","\n","        group_gradient = group.loc[:, gradient_cols]\n","\n","        group_gradient = gradient(group_gradient, depth_col)\n","\n","        group_aug = pd.concat([group_shift, group_gradient], axis=1)\n","\n","        df_aug_groups.append(group_aug)\n","\n","    return pd.concat(df_aug_groups)\n","\n","\n","def score(y_true, y_pred, scoring_matrix):\n","    \"\"\"\n","    Competition scoring function.\n","\n","    Parameters\n","    ----------\n","    y_true : pandas.Series\n","        Ground truth (correct) target values.\n","    y_pred : pandas.Series\n","        Estimated targets as returned by a classifier.\n","    scoring_matrix : numpy.array\n","        Competition scoring matrix.\n","\n","    Returns\n","    ----------\n","    float\n","        2020 FORCE ML lithology competition custome score.\n","\n","    \"\"\"\n","    S = 0.0\n","\n","    for true_val, pred_val in zip(y_true, y_pred):\n","        S -= scoring_matrix[true_val, pred_val]\n","\n","    return S/y_true.shape[0]\n","\n","\n","def show_evaluation(y_true, y_pred):\n","    \"\"\"\n","    Print model performance and evaluation.\n","\n","    Parameters\n","    ----------\n","    y_true : pandas.Series\n","        Ground truth (correct) target values.\n","    y_pred: pandas.Series\n","        Estimated targets as returned by a classifier.\n","\n","    \"\"\"\n","    print(f'Competition score: {score(y_true, y_pred)}')\n","    print(f'Accuracy: {accuracy_score(y_true, y_pred)}')\n","    print(f'F1: {f1_score(y_true, y_pred, average=\"weighted\")}')\n","\n","\n","def build_encoding_map(series):\n","    \"\"\"\n","    Build dictionary with the mapping of series unique values to encoded\n","    values.\n","\n","    Parameters\n","    ----------\n","    series : pandas.Series\n","        Series with categories to be encoded.\n","\n","    Returns\n","    -------\n","    mapping : dict\n","        Dictionary mapping unique categories in series to encoded values.\n","\n","    See Also\n","    --------\n","    label_encode_columns : Label encode a dataframe categorical columns.\n","\n","    \"\"\"\n","    unique_values = series.unique()\n","\n","    mapping = {original: encoded\n","               for encoded, original in enumerate(unique_values)\n","               if original is not np.nan}\n","\n","    return mapping\n","\n","\n","def label_encode_columns(df, cat_cols, mappings):\n","    \"\"\"\n","    Label encode a dataframe categorical columns.\n","\n","    Parameters\n","    ----------\n","    df : pandas.DataFrame\n","        Dataframe with columns to be encoded.\n","    cat_cols: list of str\n","        Column names to be encoded.\n","    mappings: dict of dict\n","        Dictionary containing a key-value mapping for each column to be\n","        encoded.\n","\n","    Returns\n","    -------\n","    df : pandas.DataFrame\n","        Dataframe with the encoded columns added and the `cat_cols` removed.\n","    encoded_col_names: list of str\n","        Encoded column names.\n","\n","    See Also\n","    --------\n","    build_encoding_map : Build a series encoding mapping.\n","\n","    \"\"\"\n","    df = df.copy()\n","\n","    encoded_col_names = []\n","    for col in cat_cols:\n","        new_col = f'{col}_encoded'\n","        encoded_col_names.append(new_col)\n","\n","        df[new_col] = df[col].map(mappings[col])\n","\n","        df.drop(col, axis=1, inplace=True)\n","\n","    return df, encoded_col_names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5S-CRBbGc02"},"source":["# Target maps"]},{"cell_type":"code","metadata":{"id":"pKPoKfl6Geev"},"source":["KEYS_TO_ORDINAL = {\n","    30000: 0,\n","    65030: 1,\n","    65000: 2,\n","    80000: 3,\n","    74000: 4,\n","    70000: 5,\n","    70032: 6,\n","    88000: 7,\n","    86000: 8,\n","    99000: 9,\n","    90000: 10,\n","    93000: 11\n","    }\n","\n","\n","KEYS_TO_LITHOLOGY = {30000: 'Sandstone',\n","                     65030: 'Sandstone/Shale',\n","                     65000: 'Shale',\n","                     80000: 'Marl',\n","                     74000: 'Dolomite',\n","                     70000: 'Limestone',\n","                     70032: 'Chalk',\n","                     88000: 'Halite',\n","                     86000: 'Anhydrite',\n","                     99000: 'Tuff',\n","                     90000: 'Coal',\n","                     93000: 'Basement'}\n","\n","ORDINAL_TO_KEYS = {value: key for key, value in  KEYS_TO_ORDINAL.items()}\n","\n","ORDINAL_TO_LITHOLOGY = {}\n","for ordinal_key, key in ORDINAL_TO_KEYS.items():\n","    ORDINAL_TO_LITHOLOGY[ordinal_key] = KEYS_TO_LITHOLOGY[key]\n","\n","LITHOLOGY_TO_ORDINAL = {}\n","for ordinal_key, lithology in ORDINAL_TO_LITHOLOGY.items():\n","    LITHOLOGY_TO_ORDINAL[lithology] = ordinal_key"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V-_H1SeJu1ho"},"source":["# Import data\n","\n","First add a shortcut from the [google drive competition data location](https://drive.google.com/drive/folders/1GIkjq4fwgwbiqVQxYwoJnOJWVobZ91pL) to your own google drive. We will mount this drive, and access the data from it.\n","\n","We will save the results to a diffent folder, where we have write access."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fEYM1Xz2u1ht","executionInfo":{"status":"ok","timestamp":1612959763389,"user_tz":360,"elapsed":19569,"user":{"displayName":"Rafael Pinto","photoUrl":"","userId":"01213912175113627450"}},"outputId":"ef6075bf-8caa-43df-8555-664a8ff3ae14"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RpIq9zscu1hu"},"source":["#should be edited to the present working directory of the user\n","data_source = '/content/drive/My Drive/FORCE 2020 lithofacies prediction from well logs competition/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BG8lQNYu1hu"},"source":["penalty_matrix = np.load(data_source + 'penalty_matrix.npy')\n","\n","train = pd.read_csv(data_source + 'CSV_train.csv', sep=';')\n","\n","test = pd.read_csv(data_source + 'CSV_hidden_test.csv', sep=';')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dt2Oge4Fu1hv"},"source":["# Destination folder\n","out_data_dir = Path('/content/drive/My Drive/lith_pred/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kJyhGrd8PDrK"},"source":["# Train model"]},{"cell_type":"code","metadata":{"id":"AmSp3xSsu1hv"},"source":["class Model():\n","    '''\n","    class to lithology prediction\n","    '''\n","    def preprocess(self, df, cat_columns, mappings):\n","\n","        # # Drop model features\n","        # drop_cols = [\n","        #              'FORCE_2020_LITHOFACIES_CONFIDENCE',\n","        #              'SGR',\n","        #              'DTS',\n","        #              'DCAL',\n","        #              'RMIC',\n","        #              'ROPA',\n","        #              'RXO',                     \n","        #              ]\n","\n","        # # Confirm drop columns are in df\n","        # drop_cols = [col for col in drop_cols if col in df.columns]\n","\n","        # df.drop(drop_cols, axis=1, inplace=True)\n","\n","        # Label encode\n","        df, encoded_col_names = label_encode_columns(df, cat_columns, mappings)\n","        \n","        # Augment using Bestagini's functions\n","        df_preprocesed = shift_concat_gradient(df,\n","                                               'DEPTH_MD',\n","                                               'WELL',\n","                                               encoded_col_names,\n","                                               periods=1,\n","                                               fill_value=None)\n","       \n","        return df_preprocesed\n","\n","    def fit(self, X, y):\n","        split = 5\n","        skf = StratifiedKFold(n_splits=split, shuffle=True)\n","\n","        model = XGBClassifier(n_estimators=100, max_depth=10, booster='gbtree',\n","                              objective='multi:softprob', learning_rate=0.1, random_state=0,\n","                              subsample=0.9, colsample_bytree=0.9, tree_method='gpu_hist',\n","                              eval_metric='mlogloss', verbose=2020, reg_lambda=1500)\n","        \n","        models = []\n","        for fold_number, indices in enumerate(skf.split(X, y)):\n","            print(f'Fitting fold: {fold_number}')\n","            \n","            train_index, test_index = indices\n","\n","            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","            model.fit(X_train,\n","                      y_train,\n","                      early_stopping_rounds=100,\n","                      eval_set=[(X_test, y_test)],\n","                      verbose=100)\n","\n","            models.append(model)\n","\n","        return models\n","\n","    def fit_predict(self, X_train, y_train, X_pred, pred_wells, save_filename):\n","        # Fit\n","        models = self.fit(X_train, y_train)\n","\n","        # Get lithologies probabilities for each model\n","        models_proba = []\n","        for model_num, model in enumerate(models):\n","            model_proba = model.predict_proba(X_pred)\n","            model_classes = [ORDINAL_TO_LITHOLOGY[lith] for lith in model.classes_]\n","\n","            model_proba_df = pd.DataFrame(model_proba, columns=model_classes)\n","\n","            model_proba_df['MODEL'] = model_num\n","\n","\n","            # Set sample index, well, and MD\n","            pred_wells_df = pred_wells.reset_index()\n","            model_proba_df['index'] = pred_wells_df['index']\n","            model_proba_df['WELL'] = pred_wells_df['WELL']\n","\n","            md = X_pred['DEPTH_MD']\n","            md.reset_index(inplace=True, drop=True)\n","            model_proba_df['DEPTH_MD'] = md\n","\n","            models_proba.append(model_proba_df)\n","\n","        models_proba = pd.concat(models_proba, ignore_index=True)\n","\n","        # Create save directory if it doesn't exists\n","        if not save_filename.parent.is_dir():\n","            save_filename.parent.mkdir(parents=True)\n","\n","        # Save models_proba to CSV\n","        models_proba.to_csv(save_filename, index=False)\n","\n","        return models, models_proba"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B0mFUwb2xlHl"},"source":["# Prepare train data"]},{"cell_type":"code","metadata":{"id":"E9VZFXQgsevf"},"source":["# Build group of groups\n","group_of_groups = {\n","    'VIKING GP.': 'VTB GP.',\n","    'BOKNFJORD GP.': 'VTB GP.',\n","    'TYNE GP.': 'VTB GP.',\n","    'ROTLIEGENDES GP.': 'PERMIAN GP.',\n","    'ZECHSTEIN GP.': 'PERMIAN GP.',\n","    }\n","\n","train['GROUPED'] = train['GROUP']\n","train['GROUPED'].replace(group_of_groups, inplace=True)\n","\n","train.drop('GROUP', axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1y3VnHosselz"},"source":["train['FORCE_2020_LITHOFACIES_LITHOLOGY'] = train['FORCE_2020_LITHOFACIES_LITHOLOGY'].map(KEYS_TO_ORDINAL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZChXlgMwsea4"},"source":["cat_columns = ['FORMATION']\n","\n","train_mappings = {col: build_encoding_map(train[col]) for col in cat_columns}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"exqiPxpTv1k_"},"source":["# Drop columns with high percent of missing values\n","drop_cols = [\n","            'FORCE_2020_LITHOFACIES_CONFIDENCE',\n","            'SGR',\n","            'DTS',\n","            'DCAL',\n","            'RMIC',\n","            'ROPA',\n","            'RXO',                     \n","            ]\n","\n","# Confirm drop columns are in df\n","drop_cols = [col for col in drop_cols if col in train.columns]\n","\n","train.drop(drop_cols, axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1IsSu_X2vRkw"},"source":["# Use different logs per group\n","limit = 0.68\n","keep_logs_per_group = {}\n","for group_data in train.groupby('GROUPED'):\n","    group_name, group = group_data\n","    \n","    group_log_coverage = (~group.isna()).sum() / group.shape[0]\n","    \n","    cond_more_than_limit = group_log_coverage > limit\n","    \n","    keep_logs = [log for log, val in cond_more_than_limit.items() if val]\n","\n","    if 'FORMATION' not in keep_logs:\n","        keep_logs.append('FORMATION')\n","    \n","    keep_logs_per_group[group_name] = keep_logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2eM4FKCJOuJn"},"source":["# Prepare predict data"]},{"cell_type":"code","metadata":{"id":"Gn9seJRB8PAQ"},"source":["y_true = test['FORCE_2020_LITHOFACIES_LITHOLOGY'].map(KEYS_TO_ORDINAL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tpCF1AdcOyTu"},"source":["test['GROUPED'] = test['GROUP']\r\n","test['GROUPED'].replace(group_of_groups, inplace=True)\r\n","\r\n","test.drop(['GROUP', 'FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YCHKzGqdPAaL"},"source":["# Confirm drop columns are in df\r\n","test_drop_cols = [col for col in drop_cols if col in test.columns]\r\n","\r\n","test.drop(test_drop_cols, axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"39B5rtkWPSuH","executionInfo":{"status":"ok","timestamp":1612959785360,"user_tz":360,"elapsed":772,"user":{"displayName":"Rafael Pinto","photoUrl":"","userId":"01213912175113627450"}},"outputId":"f3b15e8b-6e1d-4d58-e7a1-fe6d423a117b"},"source":["test.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['WELL', 'DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'FORMATION', 'CALI',\n","       'RSHA', 'RMED', 'RDEP', 'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS',\n","       'ROP', 'DRHO', 'MUDWEIGHT', 'GROUPED'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"0bi48QyIPgGj"},"source":["# Fit predict groups"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUFP5WpAuLMg","executionInfo":{"status":"ok","timestamp":1612960267467,"user_tz":360,"elapsed":480874,"user":{"displayName":"Rafael Pinto","photoUrl":"","userId":"01213912175113627450"}},"outputId":"4bda1a5c-1bd6-48e4-e323-039eba0496e0"},"source":["models_probas_list = []\n","for group_data in test.groupby('GROUPED'):\n","    group_name, group = group_data\n","\n","    if group_name in train['GROUPED'].unique():\n","        # Select train group features\n","        keep_cols = keep_logs_per_group[group_name]\n","        group_train = train.loc[train['GROUPED']==group_name, keep_cols]\n","        group_train.drop(['GROUPED'], axis=1, inplace=True)\n","\n","        # Drop lithofacies with less than n_split samples\n","        train_group_vc = group_train['FORCE_2020_LITHOFACIES_LITHOLOGY'].value_counts()\n","\n","        for lith, count in train_group_vc.items():\n","            if count <= 5:\n","                cond = group_train['FORCE_2020_LITHOFACIES_LITHOLOGY'] != lith\n","                group_train = group_train.loc[cond, :]\n","\n","        model = Model()\n","\n","        # Define train target and features\n","        y_train = group_train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n","    \n","        X = group_train.drop('FORCE_2020_LITHOFACIES_LITHOLOGY', axis=1)\n","        \n","        # Augment train features\n","        X_train = model.preprocess(X, cat_columns, train_mappings)\n","\n","        X_train.drop('WELL', axis=1, inplace=True)\n","\n","        # Define predict features\n","        pred_keep_cols = [col for col in keep_cols if col != 'FORCE_2020_LITHOFACIES_LITHOLOGY']\n","        X_pred = group.loc[:, pred_keep_cols]\n","        X_pred.drop(['GROUPED'], axis=1, inplace=True)\n","        \n","        # Augment predict features\n","        X_pred = model.preprocess(X_pred, cat_columns, train_mappings)\n","\n","        X_pred.drop('WELL', axis=1, inplace=True)\n","\n","\n","        fn = '_'.join(group_name.lower().split())\n","        save_filename = out_data_dir / f'model_proba/grouped/00_hidden/models_proba_grouped_{fn}csv'\n","\n","        predict_group_wells = group['WELL']\n","\n","        print(f'Fitting group: {group_name}')\n","        models, models_proba = model.fit_predict(X_train,\n","                                                 y_train,\n","                                                 X_pred,\n","                                                 predict_group_wells,\n","                                                 save_filename)\n","        \n","        models_probas_list.append(models_proba)\n","\n","        # print(X_pred.shape)\n","        # print()\n","        # print(predict_group_wells.shape)\n","        # print(predict_group_wells.head())\n","        # print(predict_group_wells.tail())\n","        # print()\n","\n","\n","    else:\n","        print('Group {group_name} is in the prediction set')\n","        print('but not in the train set')\n","        print('This functionality is currently not supported')\n","        print()\n","\n","        # TODO: What happens when there is a group in the predict set but not in the train set?"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting group: BAAT GP.\n","Fitting fold: 0\n","[0]\tvalidation_0-mlogloss:1.8506\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.566197\n","Fitting fold: 1\n","[0]\tvalidation_0-mlogloss:1.85069\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.57527\n","Fitting fold: 2\n","[0]\tvalidation_0-mlogloss:1.85092\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.580474\n","Fitting fold: 3\n","[0]\tvalidation_0-mlogloss:1.85112\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.58812\n","Fitting fold: 4\n","[0]\tvalidation_0-mlogloss:1.85082\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.579933\n","Fitting group: CROMER KNOLL GP.\n","Fitting fold: 0\n","[0]\tvalidation_0-mlogloss:1.95605\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.445191\n","Fitting fold: 1\n","[0]\tvalidation_0-mlogloss:1.95666\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.453318\n","Fitting fold: 2\n","[0]\tvalidation_0-mlogloss:1.95635\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.447939\n","Fitting fold: 3\n","[0]\tvalidation_0-mlogloss:1.9555\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.444653\n","Fitting fold: 4\n","[0]\tvalidation_0-mlogloss:1.95619\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.442683\n","Fitting group: DUNLIN GP.\n","Fitting fold: 0\n","[0]\tvalidation_0-mlogloss:1.79161\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.40188\n","Fitting fold: 1\n","[0]\tvalidation_0-mlogloss:1.79137\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.398095\n","Fitting fold: 2\n","[0]\tvalidation_0-mlogloss:1.79118\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.40252\n","Fitting fold: 3\n","[0]\tvalidation_0-mlogloss:1.79082\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.397268\n","Fitting fold: 4\n","[0]\tvalidation_0-mlogloss:1.79053\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.391743\n","Fitting group: HEGRE GP.\n","Fitting fold: 0\n","[0]\tvalidation_0-mlogloss:1.73124\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.556144\n","Fitting fold: 1\n","[0]\tvalidation_0-mlogloss:1.73182\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.566973\n","Fitting fold: 2\n","[0]\tvalidation_0-mlogloss:1.73177\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.552539\n","Fitting fold: 3\n","[0]\tvalidation_0-mlogloss:1.73142\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.553557\n","Fitting fold: 4\n","[0]\tvalidation_0-mlogloss:1.73129\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.558898\n","Fitting group: HORDALAND GP.\n","Fitting fold: 0\n","[0]\tvalidation_0-mlogloss:1.70792\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.193836\n","Fitting fold: 1\n","[0]\tvalidation_0-mlogloss:1.70798\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.196601\n","Fitting fold: 2\n","[0]\tvalidation_0-mlogloss:1.70774\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.19196\n","Fitting fold: 3\n","[0]\tvalidation_0-mlogloss:1.70815\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.194341\n","Fitting fold: 4\n","[0]\tvalidation_0-mlogloss:1.70811\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.194072\n","Fitting group: NORDLAND GP.\n","Fitting fold: 0\n","[0]\tvalidation_0-mlogloss:1.6264\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.242886\n","Fitting fold: 1\n","[0]\tvalidation_0-mlogloss:1.6262\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.238736\n","Fitting fold: 2\n","[0]\tvalidation_0-mlogloss:1.62622\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.240403\n","Fitting fold: 3\n","[0]\tvalidation_0-mlogloss:1.62603\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.239307\n","Fitting fold: 4\n","[0]\tvalidation_0-mlogloss:1.62533\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.229685\n","Fitting group: PERMIAN GP.\n","Fitting fold: 0\n","[0]\tvalidation_0-mlogloss:2.06767\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.354416\n","Fitting fold: 1\n","[0]\tvalidation_0-mlogloss:2.06714\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.350648\n","Fitting fold: 2\n","[0]\tvalidation_0-mlogloss:2.06703\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.35059\n","Fitting fold: 3\n","[0]\tvalidation_0-mlogloss:2.06741\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.350215\n","Fitting fold: 4\n","[0]\tvalidation_0-mlogloss:2.06669\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.345058\n","Fitting group: ROGALAND GP.\n","Fitting fold: 0\n","[0]\tvalidation_0-mlogloss:1.77716\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.327659\n","Fitting fold: 1\n","[0]\tvalidation_0-mlogloss:1.77637\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.329513\n","Fitting fold: 2\n","[0]\tvalidation_0-mlogloss:1.77592\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.330442\n","Fitting fold: 3\n","[0]\tvalidation_0-mlogloss:1.77588\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.324125\n","Fitting fold: 4\n","[0]\tvalidation_0-mlogloss:1.77621\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.323751\n","Fitting group: SHETLAND GP.\n","Fitting fold: 0\n","[0]\tvalidation_0-mlogloss:1.73976\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.265973\n","Fitting fold: 1\n","[0]\tvalidation_0-mlogloss:1.73917\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.265585\n","Fitting fold: 2\n","[0]\tvalidation_0-mlogloss:1.73914\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.259956\n","Fitting fold: 3\n","[0]\tvalidation_0-mlogloss:1.73931\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.26279\n","Fitting fold: 4\n","[0]\tvalidation_0-mlogloss:1.73918\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.266667\n","Fitting group: VESTLAND GP.\n","Fitting fold: 0\n","[0]\tvalidation_0-mlogloss:1.68873\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.383221\n","Fitting fold: 1\n","[0]\tvalidation_0-mlogloss:1.68892\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.392254\n","Fitting fold: 2\n","[0]\tvalidation_0-mlogloss:1.68865\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.387137\n","Fitting fold: 3\n","[0]\tvalidation_0-mlogloss:1.68812\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.381276\n","Fitting fold: 4\n","[0]\tvalidation_0-mlogloss:1.6886\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.382515\n","Fitting group: VTB GP.\n","Fitting fold: 0\n","[0]\tvalidation_0-mlogloss:1.90402\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.362644\n","Fitting fold: 1\n","[0]\tvalidation_0-mlogloss:1.90435\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.359793\n","Fitting fold: 2\n","[0]\tvalidation_0-mlogloss:1.90208\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.357611\n","Fitting fold: 3\n","[0]\tvalidation_0-mlogloss:1.90153\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.353618\n","Fitting fold: 4\n","[0]\tvalidation_0-mlogloss:1.9046\n","Will train until validation_0-mlogloss hasn't improved in 100 rounds.\n","[99]\tvalidation_0-mlogloss:0.361759\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a0TzxAZ_wxXT"},"source":["models_probas = pd.concat(models_probas_list, ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhX3q9j1seQY"},"source":["cols_ordered = ['Sandstone', 'Sandstone/Shale', 'Shale', 'Marl', 'Dolomite',\n","                'Limestone', 'Chalk', 'Coal', 'Anhydrite', 'Halite', 'Basement', 'Tuff',\n","                'MODEL', 'index', 'WELL', 'DEPTH_MD'\n","               ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEp4TIUv3ayS"},"source":["models_probas = models_probas.loc[:, cols_ordered]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FkAaiogC3ajA"},"source":["models_probas.fillna(0.0, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GMQWdcKg3vO8"},"source":["# y_true"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53BRLi4e3aP4","executionInfo":{"status":"ok","timestamp":1612960603608,"user_tz":360,"elapsed":821,"user":{"displayName":"Rafael Pinto","photoUrl":"","userId":"01213912175113627450"}},"outputId":"17febf52-b639-4bd4-f2ce-e32036d48a12"},"source":["y_true.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    2\n","1    2\n","2    2\n","3    2\n","4    2\n","Name: FORCE_2020_LITHOFACIES_LITHOLOGY, dtype: int64"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"uyZldzDk4Z2h"},"source":["# Probability mean over models"]},{"cell_type":"code","metadata":{"id":"UOLPcZt-35EP"},"source":["models_probas_cumsums = models_probas.groupby('index').sum()\n","\n","models_probas_cumsums = models_probas_cumsums.loc[:, 'Sandstone':'Tuff']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlVrPbd73469"},"source":["models_len = len(models_probas['MODEL'].unique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DGkhldY_4UO1"},"source":["models_probas_mean = models_probas_cumsums / models_len"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eaQfrKH631AH"},"source":["# Lithology with highest probability per sample"]},{"cell_type":"code","metadata":{"id":"TfSlS--L4T87"},"source":["models_probas_mean.columns = [LITHOLOGY_TO_ORDINAL[col] for col in models_probas_mean.columns]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xO-vux8234tm"},"source":["y_pred = models_probas_mean.idxmax(axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e-FvGmaS4ni0"},"source":["# Score"]},{"cell_type":"code","metadata":{"id":"3ttfX1594qaA"},"source":["hidden_test_score = score(y_true, y_pred, penalty_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"446nSFLX4qMw","executionInfo":{"status":"ok","timestamp":1612960623155,"user_tz":360,"elapsed":282,"user":{"displayName":"Rafael Pinto","photoUrl":"","userId":"01213912175113627450"}},"outputId":"6d5f2bb7-a9e5-4e13-a8ce-1c2d5d725e34"},"source":["print(f'Olawale modified hidden test score is: {hidden_test_score:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Olawale modified hidden test score is: -0.5064\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f4DjWLC98eh1"},"source":[""],"execution_count":null,"outputs":[]}]}